{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwrqC1Fk5-WZ"
      },
      "source": [
        "# Analyse de texte utilisant NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfBlnB6S5l2V"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.text import Text\n",
        "import nltk"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lyPdA7sDm0U",
        "outputId": "07539e8b-cfd7-4a64-aa2f-f99ccf5df82c"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pbLZHOK6sH-"
      },
      "source": [
        "# La tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCg-nz4m637N"
      },
      "source": [
        "Le but est de transformer une phrase en vecteur de mots. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owA-myd_5o30",
        "outputId": "4ccf00ca-5564-400c-cd9d-7d80a54273be"
      },
      "source": [
        "my_string = \"Je suis ici pour étudier le Natural Language Processing, j'adore ça\"\n",
        "tokens = word_tokenize(my_string)\n",
        "tokens"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Je',\n",
              " 'suis',\n",
              " 'ici',\n",
              " 'pour',\n",
              " 'étudier',\n",
              " 'le',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " ',',\n",
              " \"j'adore\",\n",
              " 'ça']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF1k_V3tBa5_"
      },
      "source": [
        "On peut voir que la tokenization n'est pas parfaite \"j'adore\" est considéré comme une seul mot. \n",
        "on va utiliser les expressions régulières pour ne récupérer que les caractères alphanumériques de chaque phrase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51bj2aWe62Nd",
        "outputId": "662da8a2-3f96-4d78-c717-6adf2ad4de90"
      },
      "source": [
        "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
        "tokens = tokenizer.tokenize(my_string)\n",
        "tokens"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Je',\n",
              " 'suis',\n",
              " 'ici',\n",
              " 'pour',\n",
              " 'étudier',\n",
              " 'le',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " 'j',\n",
              " 'adore',\n",
              " 'ça']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91B9paOwBwG0"
      },
      "source": [
        "On peut voir que l'on a toujours des majuscules ce qui peut être un problème lors d'un comptage de notre vocabulaire. \n",
        "\n",
        "N'oubliez pas de mettre votre phrase en minuscule avant de travailler desssus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bcv0-88BGz8",
        "outputId": "e438613f-3fee-4be1-d367-3d2efe4a7234"
      },
      "source": [
        "tokens = tokenizer.tokenize(my_string.lower())\n",
        "tokens"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['je',\n",
              " 'suis',\n",
              " 'ici',\n",
              " 'pour',\n",
              " 'étudier',\n",
              " 'le',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'j',\n",
              " 'adore',\n",
              " 'ça']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4FJi0hUCdc3"
      },
      "source": [
        "# Les stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dg7lrl4EJ1f"
      },
      "source": [
        "On va supprimer ici les mots les plus fréquents et qui n'apporte pas beaucoup d'informations. Si on voit la phrase comme un signal transportant de l'information, les stopwords peuvent être vue comme du bruit. \n",
        "\n",
        "Combien de stopwords sont présent dans la langue française ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxMD8nC8B0Ug",
        "outputId": "95948ce0-0cbc-4ca5-a0f2-5dcc7c983a94"
      },
      "source": [
        "len(nltk.corpus.stopwords.words('french'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "157"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTV4QvxrEhxQ"
      },
      "source": [
        "La librairie *nltk* répertorie 157 stopwords pour la langue française !\n",
        "\n",
        "Vous pouvez voir les dix premiers mots ci-dessous."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PeMDDVcEiO3",
        "outputId": "4caa7f3f-da73-49bb-a947-052cb36e0858"
      },
      "source": [
        "nltk.corpus.stopwords.words('french')[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xhqlJ5LEydQ"
      },
      "source": [
        "On peut donc retirer les stopwords de notre phrase de base. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lo5nY9WBDko4",
        "outputId": "db90d0d1-ec9d-40a3-ad9d-dfa81c6ea4f3"
      },
      "source": [
        "[w for w in tokens if not w in list(nltk.corpus.stopwords.words('french'))]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ici', 'étudier', 'natural', 'language', 'processing', 'adore', 'ça']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsqSiU2RFfzf"
      },
      "source": [
        "Notre phrase contient seulement les mots qui ont de la valeur, il est maintenant possible de l'étudier. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7eWa8hyFt2r"
      },
      "source": [
        "# Un problème de doublon "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nv18_uP1F9gG",
        "outputId": "0da967d9-d984-42b1-d265-a6a7a1a53557"
      },
      "source": [
        "my_string = \"Julien est plus grand que toi, mais tu es plus grande que Sophie. \\\n",
        "            Julien et Sophie sont plus grands que Marie\"\n",
        "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
        "tokens = tokenizer.tokenize(my_string.lower())\n",
        "tokens = [w for w in tokens if not w in list(nltk.corpus.stopwords.words('french'))]\n",
        "\n",
        "tokens"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['julien',\n",
              " 'plus',\n",
              " 'grand',\n",
              " 'plus',\n",
              " 'grande',\n",
              " 'sophie',\n",
              " 'julien',\n",
              " 'sophie',\n",
              " 'plus',\n",
              " 'grands',\n",
              " 'marie']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL0oHk82HWIv"
      },
      "source": [
        "On peut voir que dans la phrase il y a plusieurs le même mot \"grand\" mais accordé de différentes façon. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92UryKQOHQCZ",
        "outputId": "9897aad7-fa22-4efd-b8c2-82bb59d45250"
      },
      "source": [
        "nltk.FreqDist(tokens)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'plus': 3, 'julien': 2, 'sophie': 2, 'grand': 1, 'grande': 1, 'grands': 1, 'marie': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaXrymlMHmC2"
      },
      "source": [
        "Si l'on travail le texte, on va donc considérer que \"grand\", \"grande\" et \"grands\" sont trois mots différents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0htJPcieIDhv"
      },
      "source": [
        "# Le stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA7RZ43-JxDf"
      },
      "source": [
        "Le stemming garde une racinisation des mots. Cette technique permet de regrouper des verbes conjugués et des adjectifs accordés sous la même orthographe. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Voph9oTHctB"
      },
      "source": [
        "from nltk.stem.snowball import FrenchStemmer\n",
        "\n",
        "stemmer = FrenchStemmer()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M12BE934KXoQ"
      },
      "source": [
        "N'oubliez pas d'utiliser une stemming adéquat au language utilisé. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Njy0fNx7IFpF",
        "outputId": "e7ebeab5-7fd0-4d31-edd4-a3a496751235"
      },
      "source": [
        "tokens_stem = [stemmer.stem(w) for w in tokens]\n",
        "tokens_stem"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['julien',\n",
              " 'plus',\n",
              " 'grand',\n",
              " 'plus',\n",
              " 'grand',\n",
              " 'soph',\n",
              " 'julien',\n",
              " 'soph',\n",
              " 'plus',\n",
              " 'grand',\n",
              " 'mar']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3mwgHwcIZFV",
        "outputId": "59a6034f-6cb7-4acb-cdf7-1e68d7ca912a"
      },
      "source": [
        "nltk.FreqDist(tokens_stem)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'plus': 3, 'grand': 3, 'julien': 2, 'soph': 2, 'mar': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esxD53DfKcvo"
      },
      "source": [
        "On peut voir que cette fois, \"grand\", \"grande\" et \"grands\" se retrouve sous la même forme \"grand\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7ayq_ZUKkWt"
      },
      "source": [
        "# Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-Jv_f18KrKP"
      },
      "source": [
        "Le processus de lemmatisation consiste à représenter les mots sous leur forme canonique. Par exemple pour un verbe, ce sera son infinitif. Pour un nom, son masculin singulier. \n",
        "\n",
        "La fonction de lemmatization n'existe pas dans *nltk* pour le français l'exemple qui suit sera donc en anglais."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q28YNvUpLZUq"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "Word_Lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eol6SD1kLAND",
        "outputId": "32868ee0-c4e9-4302-f616-a539bc0a5cac"
      },
      "source": [
        "my_string = \"Who let's the dogs out.\"\n",
        "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
        "tokens = tokenizer.tokenize(my_string.lower())\n",
        "tokens = [w for w in tokens if not w in list(nltk.corpus.stopwords.words())]\n",
        "print(tokens)\n",
        "tokens_lema = [Word_Lemmatizer.lemmatize(w) for w in tokens]\n",
        "print(tokens_lema)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dogs']\n",
            "['dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwUC2I0HNHbd"
      },
      "source": [
        "# Part of speach tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wQxCf_wN3TL"
      },
      "source": [
        "On peut avoir des précisions sur l'usage du mot dans la phrase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HD0ygijL-u0",
        "outputId": "d8c5a643-5d08-45ee-f59f-4e16954d3a76"
      },
      "source": [
        "my_string = \"I like hot tea\"\n",
        "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
        "tokens = tokenizer.tokenize(my_string.lower())\n",
        "nltk.pos_tag(tokens)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 'NNS'), ('like', 'VBP'), ('hot', 'JJ'), ('tea', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}