{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdYLFhM6LvGg"
      },
      "source": [
        "Objectifs :\n",
        "- Implémenter le bag of word ;\n",
        "- Implémenter le TF-IDF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1gxaxo8M1hq"
      },
      "source": [
        "## Importations des packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO5tMjlUM06N",
        "outputId": "4240ddc8-f6a8-49b2-dc73-8e31120b38db"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.text import Text\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq18nvqWM8Lu"
      },
      "source": [
        "## Importations des données\n",
        "\n",
        "Les données sont issue de [cette base de données](https://cs.nyu.edu/~kcho/DMQA/).\n",
        "\n",
        "Maintenant que nos outils sont chargés, nous allons charger nos données.\n",
        "\n",
        "Cliquez sur le lien ci-dessous :\n",
        "\n",
        "https://drive.google.com/drive/folders/12OmusfAUOcoLOCwEc--nfkKQ5eEozU45?usp=sharing\n",
        "\n",
        "Cliquer droit sur le dossier data et appuyer sur ajouter à mon drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dAmPai6M7wA"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYiTdYvFNF3J"
      },
      "source": [
        "Les données sont maintenant dans votre environnement collab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vmk_Nlc2NJ7p"
      },
      "source": [
        "import os\n",
        "print(os.listdir('gdrive/MyDrive/Exercice_1/Partie_1')[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PC-_PGWtn0D"
      },
      "source": [
        "# Création de la base de données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4Q4GATMtpsz"
      },
      "source": [
        "cmpt = 0\n",
        "dict_data = dict()\n",
        "\n",
        "for file_name in os.listdir('gdrive/MyDrive/Exercice_1/Partie_1'):\n",
        "    f = open('gdrive/MyDrive/Exercice_1/Partie_1/'+file_name, 'r')\n",
        "    lst = \"\"\n",
        "    for line in f:\n",
        "\n",
        "       line.strip()\n",
        "       line = line.replace(\"\\n\" ,'')\n",
        "       line = line.replace(\"//\" , '')\n",
        "       line = line.replace(\"/\" , '')\n",
        "       if len(line) > 0 :\n",
        "        lst += line\n",
        "\n",
        "    dict_data[cmpt] = lst\n",
        "    cmpt += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoHCpAp5vrLn"
      },
      "source": [
        "# Prétraitements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqyhQF-DvX2B"
      },
      "source": [
        "## En minuscule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZHDj0xsvZxy"
      },
      "source": [
        "dict_data_min = dict()\n",
        "for k, v in dict_data.items():\n",
        "  dict_data_min[k] = v.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXLT8VOavcY0"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfBQd71VvgP0"
      },
      "source": [
        "dict_data_token = dict()\n",
        "for k, v in dict_data_min.items():\n",
        "  tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
        "  tokens = tokenizer.tokenize(v)\n",
        "  dict_data_token[k] = tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxbEUZU_wEF2"
      },
      "source": [
        "## Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJoHkqjkwHGk"
      },
      "source": [
        "dict_data_stop = dict()\n",
        "for k, v in dict_data_token.items():\n",
        "  dict_data_stop[k] = [w for w in v if not w in list(nltk.corpus.stopwords.words())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tpo8jDw8vh2B"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR4hmThDvmrG"
      },
      "source": [
        "dict_data_stem = dict()\n",
        "st = LancasterStemmer()\n",
        "\n",
        "for index, doc in dict_data_stop.items()\n",
        "  dict_data_stem[index] = [st.stem(w) for w in doc ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of words"
      ],
      "metadata": {
        "id": "8OarYLlTg9DZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vecteur de vocabulaire"
      ],
      "metadata": {
        "id": "yN3ywXveg-a6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "LxBsVhlNhAq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of words"
      ],
      "metadata": {
        "id": "vlSvSVPZhBor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "QvMaPJUjhD42"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}